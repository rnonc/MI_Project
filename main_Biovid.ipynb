{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING and RESULT on Biovid\n",
    "- This notebook presents the steps for evaluating the optimal lambda to maximize the classification task\n",
    "- The mutual information is modeled by KNIFE : https://openreview.net/pdf?id=a43otnDilz2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision.models import resnet50, ResNet50_Weights,resnet18, ResNet18_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import data_augm,data_adapt\n",
    "from knife import KNIFE\n",
    "from log_reader import Logs\n",
    "from Dataset_processing.Biovid import gen_dataframe\n",
    "from data_loader import Dataset_Biovid_image_binary_class\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from MI_training import train,Classif\n",
    "\n",
    "device_ids = [1]\n",
    "for d in device_ids:\n",
    "    torch.cuda.set_device(d)\n",
    "    torch.cuda.empty_cache()\n",
    "    #torch.cuda.current_device()\n",
    "    torch.cuda.get_device_name()\n",
    "device = f'cuda:{device_ids[0]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify and complete \n",
    "Change the paths and the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biovid_img = '.../Biovid/sub_red_classes_img'\n",
    "biovid_annot_train = '.../your_folder/Biovid_binary/train.csv'\n",
    "biovid_annot_test = '.../your_folder/Biovid_binary/test.csv'\n",
    "save_path = '.../your_folder/models/experience_lambda/'\n",
    "\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "RESOLUTION = 112\n",
    "nb_ID = 61\n",
    "FOLD = 8\n",
    "LEARNING_RATE = 0.01\n",
    "LEARNING_RATE_FINETUNE = 0.000005\n",
    "EPOCH_PRETRAIN = 10\n",
    "EPOCH_FINETUNE = 20\n",
    "LAMBDA = [round(i*0.05,2) for i in range(44)]\n",
    "\n",
    "\n",
    "arg_MI= {'zd_dim':1000, 'zc_dim':61,'hidden_state':100, 'layers':3, 'nb_mixture':10,'tri':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute new annotation\n",
    "- follow the official Biovid part A split Train/Validation\n",
    "- add ID and video_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    pd.read_csv(biovid_annot_train)\n",
    "except:\n",
    "    gen_dataframe(Biovid_img[:-20]+'/sub_two_labels.txt',biovid_annot_train[:-10],['100914_m_39','101114_w_37',\n",
    "                                                                        '082315_w_60', '083114_w_55',\n",
    "                                                                        '083109_m_60','072514_m_27',\n",
    "                                                                        '080309_m_29', '112016_m_25',\n",
    "                                                                        '112310_m_20', '092813_w_24',\n",
    "                                                                        '112809_w_23', '112909_w_20',\n",
    "                                                                        '071313_m_41', '101309_m_48',\n",
    "                                                                        '101609_m_36', '091809_w_43',\n",
    "                                                                        '102214_w_36', '102316_w_50',\n",
    "                                                                        '112009_w_43', '101814_m_58',\n",
    "                                                                        '101908_m_61', '102309_m_61',\n",
    "                                                                        '112209_m_51', '112610_w_60',\n",
    "                                                                        '112914_w_51', '120514_w_56'])\n",
    "    print('split was created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "tr = data_augm(RESOLUTION)\n",
    "tr_test = data_adapt(RESOLUTION)\n",
    "tr_size = torchvision.transforms.Resize((RESOLUTION,RESOLUTION),antialias=True)\n",
    "\n",
    "#Train\n",
    "dataset_train = Dataset_Biovid_image_binary_class(Biovid_img,biovid_annot_train,transform = tr.transform,IDs = None,nb_image = None,nb_fold=FOLD,preload=False)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                             batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                             num_workers=0,drop_last = True) # num_workers=0 if preload is False\n",
    "\n",
    "# Validation\n",
    "dataset_test = Dataset_Biovid_image_binary_class(Biovid_img,biovid_annot_test,transform = tr_test.transform,IDs = None,nb_image = None,preload=False)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test,\n",
    "                                             batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                             num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models, Losses and optimizers\n",
    "- ResNet18 is pretrained with ImageNet and freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).to(device)\n",
    "model_affect = Classif(1,False).to(device)\n",
    "model_ID = Classif(nb_ID).to(device)\n",
    "MI = KNIFE(**arg_MI).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model_affect.parameters())+list(model_ID.parameters()),lr=LEARNING_RATE)\n",
    "optimizer_MI = torch.optim.Adam(list(MI.parameters()),lr=0.01)\n",
    "\n",
    "loss_BCE = torch.nn.BCELoss(reduction='sum')\n",
    "loss_CE = torch.nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain MI, Affect_Classifier and ID_Classifier\n",
    "- ID_classifier doesn't influences the ResNet representation (encoded_img.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_log = {'loss_CE_train':[],'loss_CE_val':[],'loss_acc_train':[],'loss_acc_val':[],'loss_acc_ID_val': [],'MI':[]}\n",
    "\n",
    "for epoch in range(EPOCH_PRETRAIN):\n",
    "    dataset_train.reset()\n",
    "    loss_task_tot = 0\n",
    "    elem_sum = 0\n",
    "    true_response_affect = 0\n",
    "    true_response_ID = 0\n",
    "    MI_loss_tot = 0\n",
    "    model_resnet.train()\n",
    "    model_affect.train()\n",
    "    model_ID.train()\n",
    "    loop_train = tqdm(loader_train,colour='BLUE')\n",
    "    for i,pack in enumerate(loop_train):\n",
    "\n",
    "        img_tensor = pack[0].to(device)\n",
    "        pain_tensor = pack[1].float().to(device)\n",
    "        ID_tensor = pack[2].to(device)\n",
    "        ID_tensor_one_hot = torch.nn.functional.one_hot(ID_tensor,61).float()\n",
    "        with torch.no_grad():\n",
    "            encoded_img  = model_resnet(img_tensor)\n",
    "\n",
    "        # UPDATE MI\n",
    "        loss_MI = MI.loss(encoded_img.detach(),ID_tensor_one_hot)\n",
    "        optimizer_MI.zero_grad()\n",
    "        loss_MI.backward()\n",
    "        optimizer_MI.step()\n",
    "        MI_loss_tot += float(loss_MI)\n",
    "        \n",
    "        # TASK Affect\n",
    "        output = model_affect(encoded_img)\n",
    "        loss_task_affect = loss_BCE(output,pain_tensor) \n",
    "        true_response_affect += float(torch.sum(output.round() == pain_tensor))\n",
    "\n",
    "        # Task ID\n",
    "        output = model_ID(encoded_img.detach())\n",
    "        loss_task_ID = loss_CE(output,ID_tensor) \n",
    "        true_response_ID += float(torch.sum(output.max(dim=-1)[1] == ID_tensor))\n",
    "\n",
    "        loss =  loss_task_ID + loss_task_affect\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        elem_sum += img_tensor.shape[0]\n",
    "        loss_task_tot += float(loss_task_affect)\n",
    "\n",
    "        loop_train.set_description(f\"Epoch [{epoch}/{EPOCH_PRETRAIN}] training\")\n",
    "        loop_train.set_postfix(loss_task = loss_task_tot/elem_sum,accuracy_pain=true_response_affect/elem_sum*100,accuracy_ID=true_response_ID/elem_sum*100,MI=MI_loss_tot/elem_sum)\n",
    "    \n",
    "    model_resnet.eval()\n",
    "    model_affect.eval()\n",
    "    model_ID.eval()\n",
    "\n",
    "    loss_task_val = 0\n",
    "    elem_sum_val = 0\n",
    "    true_response_affect_val  =0\n",
    "    true_response_ID_val = 0\n",
    "    loop_test = tqdm(loader_test,colour='GREEN')\n",
    "    for pack in loop_test:\n",
    "        img_tensor = pack[0].to(device)\n",
    "        pain_tensor = pack[1].float().to(device)\n",
    "        ID_tensor = pack[2].to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            encoded_img  = model_resnet(img_tensor)\n",
    "            # TASK Affect\n",
    "            output = model_affect(encoded_img)\n",
    "            loss_task_affect_val = loss_BCE(output,pain_tensor) \n",
    "            true_response_affect_val += float(torch.sum(output.round() == pain_tensor))\n",
    "\n",
    "            # Task ID\n",
    "            output = model_ID(encoded_img.detach())\n",
    "            loss_task_ID_val = loss_CE(output,ID_tensor) \n",
    "            true_response_ID_val += float(torch.sum(output.max(dim=-1)[1] == ID_tensor))\n",
    "\n",
    "        elem_sum_val += img_tensor.shape[0]\n",
    "        loss_task_val += float(loss_task_affect_val)\n",
    "        loop_test.set_description(f\"Test\")\n",
    "        loop_test.set_postfix(loss_task = loss_task_val/elem_sum_val,accuracy_pain=true_response_affect_val/elem_sum_val*100,accuracy_ID=true_response_ID_val/elem_sum_val*100)\n",
    "\n",
    "    dic_log['loss_CE_train'].append(loss_task_tot/elem_sum)\n",
    "    dic_log['MI'].append(MI_loss_tot/elem_sum)\n",
    "    dic_log['loss_acc_train'].append(true_response_affect/elem_sum*100)\n",
    "    dic_log['loss_CE_val'].append(loss_task_val/elem_sum_val)\n",
    "    dic_log['loss_acc_val'].append(true_response_affect_val/elem_sum_val*100)\n",
    "    dic_log['loss_acc_ID_val'].append(true_response_ID_val/elem_sum_val*100)\n",
    "    torch.save(model_resnet.state_dict(),save_path+'encoder.pt')\n",
    "    torch.save(model_ID.state_dict(),save_path+'ID.pt')\n",
    "    torch.save(model_affect.state_dict(),save_path+'Affect.pt')\n",
    "    torch.save(MI.state_dict(),save_path+'MI.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "- a training loop for each lambda value\n",
    "- all is unfreezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lamb in LAMBDA:\n",
    "        model_resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).to(device)\n",
    "        model_resnet.load_state_dict(torch.load(save_path+'encoder.pt'))\n",
    "\n",
    "        model_affect = Classif(1,False).to(device)\n",
    "        model_affect.load_state_dict(torch.load(save_path+'Affect.pt'))\n",
    "\n",
    "        model_ID = Classif(nb_ID).to(device)\n",
    "        model_ID.load_state_dict(torch.load(save_path+'ID.pt'))\n",
    "\n",
    "        MI = KNIFE(**arg_MI).to(device)\n",
    "        MI.load_state_dict(torch.load(save_path+'MI.pt'))\n",
    "\n",
    "        train(model_resnet,MI,model_affect,model_ID,loader_train,loader_test,device,lamb=lamb,EPOCH=EPOCH_FINETUNE,lr=LEARNING_RATE_FINETUNE,save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = Logs(save_path,select=LAMBDA,colormap='brg')\n",
    "L.plot()\n",
    "L.lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.curve('loss_acc_val',degree=2,res=300,color='orange')\n",
    "#L.curve('loss_acc_ID_train',degree=-1,res=300,color='green')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
